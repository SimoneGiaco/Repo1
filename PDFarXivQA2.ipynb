{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcda717-57ce-4ccb-a138-5feac3c1c031",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "path = hf_hub_download(repo_id=r'microsoft/Phi-3-mini-4k-instruct-gguf', \n",
    "                       filename='Phi-3-mini-4k-instruct-q4.gguf', \n",
    "                       token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb6ebc2-ea8d-4375-abbd-9a4a4e328ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter \n",
    "from langchain_community.document_loaders import PyPDFLoader, ArxivLoader\n",
    "import chromadb\n",
    "import gradio as gr\n",
    "\n",
    "llm = Llama(\n",
    "  model_path=str(path),  #path to the GGUF file\n",
    "  n_ctx=4096,  #max context length\n",
    "  n_threads=8, #number of CPU threads. Tune to improve performance\n",
    "  n_gpu_layers=0, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0e0632-2286-40b7-b59d-f4f1ff78e5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function which returns the answer of the model to a query\n",
    "def llm_output(query: str)->str:\n",
    "    output = llm(\n",
    "      f\"<|user|>\\n{query}<|end|>\\n<|assistant|>\", #format for the prompt\n",
    "      max_tokens=350,  #maximum number of tokens generated by the model\n",
    "      stop=[\"<|end|>\"], \n",
    "      echo=False,  # Whether to echo the prompt\n",
    "      temperature=0.3\n",
    "    )\n",
    "    return output['choices'][0]['text'] #output is a dictionary. We just retain the text of the provided answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af79459e-1f9b-4adf-9194-59a9d4b270dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to handle PDF RAG\n",
    "\n",
    "# We load a pdf file with PyPDFLoader. The function returns a list of documents (one per page)\n",
    "def document_loader(file)->list:\n",
    "    loader=PyPDFLoader(file)\n",
    "    loaded_document=loader.load_and_split()\n",
    "    return loaded_document \n",
    "\n",
    "#We split the loaded pdf into chunks of text. Returns a list of documents (one for each chunk)\n",
    "def text_splitter(data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=40,\n",
    "        separators=[\"\\n\\n\", \"\"], #this instead of the default separators to improve chunk overlaps\n",
    "        length_function=len,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    return chunks\n",
    "\n",
    "#Returns the 3 chunks of text most relevant for the query, as determined by chromadb\n",
    "def doc_database(chunks,query: str)->list:\n",
    "    \n",
    "    #chunk_list retains for each chunk only the text (page_content) and removes metadata. To be used for the Chroma collection\n",
    "    chunk_list=list(map(lambda x: x.page_content, chunks))  \n",
    "    \n",
    "    #List of ids for the Chroma collection (list of strings)\n",
    "    ids=list(map(str,range(len(chunks))))\n",
    "    \n",
    "    client=chromadb.Client()\n",
    "    collection=client.get_or_create_collection(\n",
    "        name='doc_collection',\n",
    "        configuration={\n",
    "            \"hnsw\": {\n",
    "                \"space\": \"cosine\" #The default metric is L^2 but cosine is more suited for text similarity\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    collection.upsert(ids=ids, documents=chunk_list)\n",
    "    results= collection.query(query_texts=[query], n_results=5)  \n",
    "    return results['documents'][0] #results['documents'] is a list with a sinle element, which is the list of relevant chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2277cf-3a9d-406d-8ee6-4baba0c2e929",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to handle arXiv RAG\n",
    "\n",
    "# Similar to the document_loader function defined previously. We load a file from the arXiv website by providing the arxiv number (xxxx.xxxxx) as input.\n",
    "def arxiv_loader(arxiv_num: str)->list:\n",
    "    loader=ArxivLoader(arxiv_num)\n",
    "    loaded_document=loader.load()\n",
    "    return loaded_document \n",
    "\n",
    "#Variant of the text_splitter function defined previously to handle arxiv documents\n",
    "def arxiv_text_splitter(data):\n",
    "    text=data[0].page_content  #The output of arxiv_loader is a list with one element. We extract the content which is a string\n",
    "    text_splitter = CharacterTextSplitter( #since text is of type str, without separators, we can use CharacterTextSplitter\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=40,\n",
    "        separator='', \n",
    "        length_function=len,\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "#Variant of the doc_database function tailored for the arxiv documents.\n",
    "def arxiv_database(chunks: list, query: str)->list:\n",
    "    \n",
    "    #We do not need to preprocess chunks since in this case the output of arxiv_text_splitter is already a list of strings\n",
    "    #List of ids for the Chroma collection (list of strings)\n",
    "    ids=list(map(str,range(len(chunks))))\n",
    "    \n",
    "    client=chromadb.Client()\n",
    "    collection=client.get_or_create_collection(\n",
    "        name='arxiv_collection',\n",
    "        configuration={\n",
    "            \"hnsw\": {\n",
    "                \"space\": \"cosine\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    collection.upsert(ids=ids, documents=chunks)\n",
    "    results= collection.query(query_texts=[query], n_results=5)  \n",
    "    return results['documents'][0] #results['documents'] is a list with a sinle element, which is the list of relevant chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd49934-2df6-44b9-81dd-d8131e8f56db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function which generates the model output. Works for both PDF and arXiv documents\n",
    "\n",
    "def text_question(query: str, database: list)->str:\n",
    "    '''\n",
    "    inputs:\n",
    "           query (the question we ask to the chatbot)\n",
    "           database (the list of relevant text chunks from doc_database)\n",
    "    outputs the answer of the chatbot\n",
    "    '''\n",
    "    #We append to the prompt template the text contained in database\n",
    "    text=''\n",
    "    for i in range(len(database)):\n",
    "        text += database[i]\n",
    "        \n",
    "    template='Considering the following text, can you explain'+query+'text:'+text   \n",
    "    output=llm_output(template)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a59630-657d-4d88-b6c0-3025a893e9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradio application\n",
    "\n",
    "#Function which combines all the previous ones to generate a chatbot answer about the pdf file from a query\n",
    "def pdf_qa(query: str, file)->str:\n",
    "    data = document_loader(file)\n",
    "    chunks = text_splitter(data)\n",
    "    database = doc_database(chunks,query)\n",
    "    return text_question(query, database)\n",
    "\n",
    "#Function which combines all the previous ones to generate a chatbot answer about the arxiv file from a query\n",
    "def arxiv_qa(query: str, arxiv_num: str)->str:\n",
    "    data = arxiv_loader(arxiv_num)\n",
    "    chunks = arxiv_text_splitter(data)\n",
    "    database = arxiv_database(chunks,query)\n",
    "    return text_question(query, database)\n",
    "\n",
    "\n",
    "#Code for gradio App. We include two buttons b1 and b2 which determine whether we have to process the uploaded pdf or the arXiv file respectively.\n",
    "demo = gr.Blocks()\n",
    "\n",
    "with demo:\n",
    "    gr.Markdown(\n",
    "    \"\"\"\n",
    "    # Document QA\n",
    "    Upload a PDF File and press 'File upload' or provide the arXiv number and press 'File from arXiv'. The bot will answer your question.\n",
    "    \"\"\")\n",
    "    query  = gr.Textbox(label=\"Input Question\", lines=2, placeholder=\"Type your question here...\")\n",
    "    file   = gr.File(label=\"Upload PDF File\", file_count=\"single\", file_types=['.pdf'], type=\"filepath\")  # Drag and drop file upload\n",
    "    text   = gr.Textbox(label=\"arXiv number\", lines=2, placeholder=\"Enter a valid arXiv number\")\n",
    "    answer = gr.Textbox(label=\"Output\")\n",
    "\n",
    "    b1 = gr.Button(\"File upload\")\n",
    "    b2 = gr.Button(\"File from arXiv\")\n",
    "\n",
    "    b1.click(pdf_qa, inputs=[query, file], outputs=answer)\n",
    "    b2.click(arxiv_qa, inputs=[query, text], outputs=answer)\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
